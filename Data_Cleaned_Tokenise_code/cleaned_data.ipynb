{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For C language \n",
    "import os\n",
    "\n",
    "def clean_tokenized_code(tokenized_code):\n",
    "    # Define which tokens to keep\n",
    "    tokens_to_keep = {'NEWLINE:', 'ID:', 'SEMI:', 'LBRACE:', 'RBRACE:', 'LPAREN:', 'RPAREN:', 'NUMBER:', 'STRING:', 'TIMES:', 'PLUS:', 'DIVIDE:'}\n",
    "    \n",
    "    # Create a cleaned list of tokens\n",
    "    cleaned_code = []\n",
    "    \n",
    "    for token in tokenized_code:\n",
    "        if token in tokens_to_keep:\n",
    "            cleaned_code.append(token)\n",
    "        else:\n",
    "            # Retain the actual code values (e.g., variable names, function names, etc.)\n",
    "            cleaned_code.append(token)\n",
    "    \n",
    "    # Remove extra white spaces\n",
    "    cleaned_code = ' '.join(cleaned_code)\n",
    "    \n",
    "    return cleaned_code\n",
    "\n",
    "def clean_files_in_folder(input_folder, output_folder):\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):  # Assuming your files have .txt extension\n",
    "            input_file_path = os.path.join(input_folder, filename)\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            # Read the content of the file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:  # Specify encoding\n",
    "                code = file.read().split()\n",
    "                \n",
    "            # Clean the tokenized code\n",
    "            cleaned_code = clean_tokenized_code(code)\n",
    "            \n",
    "            # Write the cleaned code to the output file\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:  # Specify encoding\n",
    "                file.write(cleaned_code)\n",
    "\n",
    "    # Print completion message\n",
    "    print(\"Task complete: All files have been cleaned and saved.\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = r\"E:\\codes\\sumit\\SEM_PROJ_4\\Token_output\\C++_tokenise_data\\C++\"  # Use raw string\n",
    "output_folder = r\"E:\\codes\\sumit\\SEM_PROJ_4\\Cleaned_Tokenise_data\\C++_Cleaned_Tokenise_data\\C++\"  # Use raw string\n",
    "clean_files_in_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For C++ language \n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_tokenized_code_via_regex(tokenized_code):\n",
    "    # Define the regex patterns to match the tokens to keep\n",
    "    token_patterns = [\n",
    "        r'Token.Comment.Preproc', r'Token.Comment.Single', r'Token.Text.Whitespace', \n",
    "        r'Token.Comment.PreprocFile', r'Token.Keyword', r'Token.Name.Namespace', \n",
    "        r'Token.Punctuation', r'Token.Keyword.Type', r'Token.Name.Function', \n",
    "        r'Token.Name', r'Token.Operator', r'Token.Literal.Number.Integer', \n",
    "        r'Token.Literal.String', r'Token.Literal.Number.Float', r'Token.Name.Class'\n",
    "    ]\n",
    "    \n",
    "    # Compile the patterns into a single regex pattern\n",
    "    combined_pattern = re.compile(r'|'.join(token_patterns))\n",
    "    \n",
    "    # Create a cleaned list of tokens\n",
    "    cleaned_code = []\n",
    "    \n",
    "    # Iterate over the tokens and filter using regex\n",
    "    for line in tokenized_code:\n",
    "        token_match = combined_pattern.search(line)\n",
    "        if token_match:\n",
    "            cleaned_code.append(line)\n",
    "            # Debug print to confirm matching tokens\n",
    "            print(f'Adding matched token: {line}')\n",
    "    \n",
    "    # Join the cleaned code into a single string\n",
    "    cleaned_code_str = ' '.join(cleaned_code)\n",
    "    \n",
    "    return cleaned_code_str\n",
    "\n",
    "def clean_files_in_folder_via_regex(input_folder, output_folder):\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):  # Assuming your files have .txt extension\n",
    "            input_file_path = os.path.join(input_folder, filename)\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            # Read the content of the file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:  # Specify encoding\n",
    "                code = file.read().split('\\n')\n",
    "                \n",
    "            # Debug print to verify file reading\n",
    "            print(f'Reading file: {filename}, with {len(code)} lines of tokens')\n",
    "            \n",
    "            # Clean the tokenized code via regex\n",
    "            cleaned_code = clean_tokenized_code_via_regex(code)\n",
    "            \n",
    "            # Debug print to verify cleaned code length\n",
    "            print(f'Writing cleaned code with {len(cleaned_code.split())} tokens to file: {output_file_path}')\n",
    "            \n",
    "            # Write the cleaned code to the output file\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:  # Specify encoding\n",
    "                file.write(cleaned_code)\n",
    "\n",
    "    # Print completion message\n",
    "    print(\"Task complete: All files have been cleaned and saved.\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = r\"E:\\codes\\sumit\\SEM_PROJ_4\\Token_output\\C++_tokenise_data\\C++ 7\"  # Use raw string\n",
    "output_folder = r\"E:\\codes\\sumit\\SEM_PROJ_4\\Cleaned_Tokenise_data\\C++_Cleaned_Tokenise_data\\C++ 7\"  # Use raw string\n",
    "clean_files_in_folder_via_regex(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
